{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Data ðŸ“¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "dataset_df = pd.read_csv(\"../data/generic_changed_dataset.csv\")\n",
    "le = LabelEncoder()\n",
    "type_encoded = le.fit_transform(dataset_df[\"type\"])  # Now each type is a unique int\n",
    "num_types = len(le.classes_)  # Number of unique types\n",
    "print(num_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 768)\n",
      "(2000, 384)\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings without training it again from the dataset_df\n",
    "X_distil_bert = np.load(\"distil_bert_embeddings.npy\")\n",
    "print(X_distil_bert.shape)\n",
    "X_sbert = np.load(\"sbert_embeddings.npy\")\n",
    "print(X_sbert.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Embedding Type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Train Shape",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Val Shape",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Test Shape",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "3f1f7cf0-bb23-444b-96eb-599bd674b055",
       "rows": [
        [
         "0",
         "DistilBERT",
         "(800, 768)",
         "(600, 768)",
         "(600, 768)"
        ],
        [
         "1",
         "SBERT",
         "(800, 384)",
         "(600, 384)",
         "(600, 384)"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embedding Type</th>\n",
       "      <th>Train Shape</th>\n",
       "      <th>Val Shape</th>\n",
       "      <th>Test Shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DistilBERT</td>\n",
       "      <td>(800, 768)</td>\n",
       "      <td>(600, 768)</td>\n",
       "      <td>(600, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SBERT</td>\n",
       "      <td>(800, 384)</td>\n",
       "      <td>(600, 384)</td>\n",
       "      <td>(600, 384)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Embedding Type Train Shape   Val Shape  Test Shape\n",
       "0     DistilBERT  (800, 768)  (600, 768)  (600, 768)\n",
       "1          SBERT  (800, 384)  (600, 384)  (600, 384)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_BERT_train, X_BERT_test, y1_BERT_train, y1_BERT_test, y2_BERT_train, y2_BERT_test = train_test_split(X_distil_bert, dataset_df[\"labels\"], type_encoded, test_size=0.6, random_state=42) # Train split\n",
    "X_BERT_val, X_BERT_test, y1_BERT_val, y1_BERT_test, y2_BERT_val, y2_BERT_test = train_test_split(X_BERT_test, y1_BERT_test, y2_BERT_test, test_size=0.5, random_state=42) # Validation and test split\n",
    "\n",
    "X_SBERT_train, X_SBERT_test, y1_SBERT_train, y1_SBERT_test, y2_SBERT_train, y2_SBERT_test = train_test_split(X_sbert, dataset_df[\"labels\"], type_encoded, test_size=0.6, random_state=42) # Train split\n",
    "X_SBERT_val, X_SBERT_test, y1_SBERT_val, y1_SBERT_test, y2_SBERT_val, y2_SBERT_test = train_test_split(X_SBERT_test, y1_SBERT_test, y2_SBERT_test, test_size=0.5, random_state=42) # Validation and test split\n",
    "\n",
    "\n",
    "# Put the shapes into a table for easy comparison\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Embedding Type\": [\n",
    "            \"DistilBERT\",\n",
    "            \"SBERT\",\n",
    "        ],\n",
    "        \"Train Shape\": [\n",
    "            X_BERT_train.shape,\n",
    "            X_SBERT_train.shape,\n",
    "        ],\n",
    "        \"Val Shape\": [\n",
    "            X_BERT_val.shape,\n",
    "            X_SBERT_val.shape,\n",
    "        ],\n",
    "        \"Test Shape\": [\n",
    "            X_BERT_test.shape,\n",
    "            X_SBERT_test.shape,\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_data = {\n",
    "    \"DistilBERT\": {\n",
    "        \"X_train\": X_BERT_train,\n",
    "        \"X_val\": X_BERT_val,\n",
    "        \"X_test\": X_BERT_test,\n",
    "        \"y1_train\": y1_BERT_train,\n",
    "        \"y1_val\": y1_BERT_val,\n",
    "        \"y1_test\": y1_BERT_test,\n",
    "        \"y2_train\": y2_BERT_train,\n",
    "        \"y2_val\": y2_BERT_val,\n",
    "        \"y2_test\": y2_BERT_test,\n",
    "    },\n",
    "    \"SBERT\": {\n",
    "        \"X_train\": X_SBERT_train,\n",
    "        \"X_val\": X_SBERT_val,\n",
    "        \"X_test\": X_SBERT_test,\n",
    "        \"y1_train\": y1_SBERT_train,\n",
    "        \"y1_val\": y1_SBERT_val,\n",
    "        \"y1_test\": y1_SBERT_test,\n",
    "        \"y2_train\": y2_SBERT_train,\n",
    "        \"y2_val\": y2_SBERT_val,\n",
    "        \"y2_test\": y2_SBERT_test,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import scipy\n",
    "\n",
    "\n",
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, embedding, labels, types):\n",
    "        self.X = torch.tensor(embedding, dtype=torch.float32)\n",
    "        # For binary task, we use float labels\n",
    "        self.labels = torch.tensor(\n",
    "            labels.values if hasattr(labels, \"values\") else labels, dtype=torch.float32\n",
    "        )\n",
    "        # For multi-class, targets should be long (integer encoded)\n",
    "        self.types = torch.tensor(\n",
    "            types.values if hasattr(types, \"values\") else types, dtype=torch.long\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.labels[idx], self.types[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskNet(nn.Module):\n",
    "    def __init__(self, feature_dimension, num_types):\n",
    "        super(MultiTaskNet, self).__init__()\n",
    "        # Shared layers\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(feature_dimension, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),  # Regularization for preventing overfitting\n",
    "        )\n",
    "        # Head for binary scam label prediction\n",
    "        self.label_head = nn.Linear(512, 1)  # output logit for binary classification\n",
    "\n",
    "        # Head for multi-class scam type prediction\n",
    "        self.type_head = nn.Linear(512, num_types)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_rep = self.shared(x)\n",
    "        # For binary classification, weâ€™ll use BCEWithLogitsLoss, so no sigmoid here.\n",
    "        label_logits = self.label_head(shared_rep)\n",
    "        # For type classification, output logits (to be used with CrossEntropyLoss)\n",
    "        type_logits = self.type_head(shared_rep)\n",
    "        return label_logits, type_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class MutltiTaskModel:\n",
    "    def __init__(self, X_train, y1_train, y2_train, num_types, batch_size=32, lr=1e-3):\n",
    "        self.model = MultiTaskNet(X_train.shape[1], num_types)\n",
    "\n",
    "        # Determine device (cuda, mps, or cpu)\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion_label = nn.BCEWithLogitsLoss()\n",
    "        self.criterion_type = nn.CrossEntropyLoss()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dim = X_train.shape[1]\n",
    "\n",
    "        # Create DataLoader for training\n",
    "        self.train_dataset = MultiTaskDataset(X_train, y1_train, y2_train)\n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset, batch_size=batch_size, shuffle=True\n",
    "        )\n",
    "\n",
    "        # Lists to store losses for plotting\n",
    "        self.train_label_losses = []\n",
    "        self.train_type_losses = []\n",
    "\n",
    "        # For saving the best model\n",
    "        self.best_model_weights = None\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"\n",
    "        Prints the model summary.\n",
    "        \"\"\"\n",
    "        self.model.to(\"cpu\")\n",
    "        summary(self.model, (self.batch_size, self.input_dim))\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        best_loss = float(\"inf\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            total_loss_label = 0.0\n",
    "            total_loss_type = 0.0\n",
    "            num_batches = 0\n",
    "\n",
    "            for inputs, labels, types in self.train_loader:\n",
    "                inputs, labels, types = (\n",
    "                    inputs.to(self.device),\n",
    "                    labels.to(self.device),\n",
    "                    types.to(self.device),\n",
    "                )\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                label_logits, type_logits = self.model(inputs)\n",
    "\n",
    "                loss_label = self.criterion_label(label_logits.view(-1), labels)\n",
    "                loss_type = self.criterion_type(type_logits, types)\n",
    "\n",
    "                loss = loss_label + loss_type\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss_label += loss_label.item()\n",
    "                total_loss_type += loss_type.item()\n",
    "                num_batches += 1\n",
    "\n",
    "            # Calculate average loss for the epoch\n",
    "            avg_loss_label = total_loss_label / num_batches\n",
    "            avg_loss_type = total_loss_type / num_batches\n",
    "            total_loss = avg_loss_label + avg_loss_type\n",
    "\n",
    "            # Store losses for plotting\n",
    "            self.train_label_losses.append(avg_loss_label)\n",
    "            self.train_type_losses.append(avg_loss_type)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "                print(f\"Average Loss (Label): {avg_loss_label:.4f}\")\n",
    "                print(f\"Average Loss (Type): {avg_loss_type:.4f}\")\n",
    "                print(f\"Total Loss: {total_loss:.4f}\\n\")\n",
    "\n",
    "            # Save best model weights\n",
    "            if total_loss < best_loss:\n",
    "                best_loss = total_loss\n",
    "                self.best_model_weights = self.model.state_dict()\n",
    "\n",
    "        print(\"Training complete.\")\n",
    "\n",
    "    def plot_losses(self):\n",
    "        \"\"\"\n",
    "        Plots the losses for label and type predictions during training.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.train_label_losses, label=\"Label Loss\")\n",
    "        plt.plot(self.train_type_losses, label=\"Type Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training Losses\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Makes predictions for the given inputs X.\n",
    "\n",
    "        Parameters:\n",
    "            X (numpy array or tensor): Input features.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (predicted_label, predicted_type) where predicted_label is the binary prediction (0 or 1)\n",
    "                   and predicted_type is the predicted class for multi-class task.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "            label_logits, type_logits = self.model(inputs)\n",
    "            label_pred = (\n",
    "                (torch.sigmoid(label_logits) >= 0.5).cpu().numpy().astype(int)\n",
    "            )  # Convert to 0 or 1 instead of floating point from sigmoid\n",
    "            type_pred = torch.argmax(type_logits, dim=1).cpu().numpy()\n",
    "\n",
    "        return label_pred, type_pred\n",
    "\n",
    "    def evaluate(self, X, y1, y2):\n",
    "        \"\"\"\n",
    "        Evaluates the model on the provided dataset and computes various performance metrics.\n",
    "\n",
    "        Parameters:\n",
    "            X (array-like): Input features.\n",
    "            y1 (array-like): True labels for binary classification.\n",
    "            y2 (array-like): True labels for multi-class classification.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A nested tuple containing:\n",
    "                - accuracy (float): Overall accuracy of the model.\n",
    "                - precision (float): Precision score.\n",
    "                - recall (float): Recall score.\n",
    "                - f1 (float): F1 score.\n",
    "                - conf_matrix (array-like): Confusion matrix.\n",
    "                - class_report (str): Text summary of the precision, recall, and f1-score for each class.\n",
    "        \"\"\"\n",
    "        y1_pred, y2_pred = self.predict(X)\n",
    "\n",
    "        # Binary classification metrics\n",
    "        accuracy_label = accuracy_score(y1, y1_pred)\n",
    "        precision_label = precision_score(y1, y1_pred, zero_division=0)\n",
    "        recall_label = recall_score(y1, y1_pred, zero_division=0)\n",
    "        f1_label = f1_score(y1, y1_pred, zero_division=0)\n",
    "        conf_matrix_label = confusion_matrix(y1, y1_pred)\n",
    "\n",
    "        # Multi-class classification metrics\n",
    "        accuracy_type = accuracy_score(y2, y2_pred)\n",
    "        precision_type = precision_score(\n",
    "            y2, y2_pred, average=\"weighted\", zero_division=0\n",
    "        )\n",
    "        recall_type = recall_score(y2, y2_pred, average=\"weighted\", zero_division=0)\n",
    "        f1_type = f1_score(y2, y2_pred, average=\"weighted\", zero_division=0)\n",
    "        conf_matrix_type = confusion_matrix(y2, y2_pred)\n",
    "\n",
    "        return (\n",
    "            accuracy_label,\n",
    "            precision_label,\n",
    "            recall_label,\n",
    "            f1_label,\n",
    "            conf_matrix_label,\n",
    "        ), (accuracy_type, precision_type, recall_type, f1_type, conf_matrix_type)\n",
    "\n",
    "    def plot_confusion_matrix(self, X, y1, y2, labels):\n",
    "        \"\"\"\n",
    "        Plots the confusion matrix for the model's predictions.\n",
    "\n",
    "        Parameters:\n",
    "            X (array-like): Input features.\n",
    "            y1 (array-like): True labels for binary classification.\n",
    "            y2 (array-like): True labels for multi-class classification.\n",
    "            labels (list): List of label names to be used in the plot axes.\n",
    "        \"\"\"\n",
    "        y1_pred, y2_pred = self.predict(X)\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.heatmap(\n",
    "            confusion_matrix(y1, y1_pred),\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=[\"Not Scam\", \"Scam\"],\n",
    "            yticklabels=[\"Not Scam\", \"Scam\"],\n",
    "        )\n",
    "        plt.xlabel(\"Predicted Labels\")\n",
    "        plt.ylabel(\"True Labels\")\n",
    "        plt.title(\"Binary Classification\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.heatmap(\n",
    "            confusion_matrix(y2, y2_pred),\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=labels,\n",
    "            yticklabels=labels,\n",
    "        )\n",
    "        plt.xlabel(\"Predicted Labels\")\n",
    "        plt.ylabel(\"True Labels\")\n",
    "        plt.title(\"Multi-Class Classification\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Multi-Task Model for DistilBERT...\n",
      "Training Multi-Task Model for SBERT...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the multi_task_model for the different embeddings\n",
    "multi_task_models = {}\n",
    "\n",
    "for embedding, data in embedding_data.items():\n",
    "    print(f\"Training Multi-Task Model for {embedding}...\")\n",
    "\n",
    "    # Convert sparse matrix to dense if needed\n",
    "    X_train = (\n",
    "        data[\"X_train\"].toarray()\n",
    "        if scipy.sparse.issparse(data[\"X_train\"])\n",
    "        else data[\"X_train\"]\n",
    "    )\n",
    "\n",
    "    multi_task_model = MutltiTaskModel(\n",
    "        X_train, data[\"y1_train\"], data[\"y2_train\"], num_types=len(le.classes_)\n",
    "    )\n",
    "\n",
    "    # Add the model to the dictionary\n",
    "    multi_task_models[embedding] = multi_task_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 22:10:26,258\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2025-03-07 22:10:26,285\tWARNING callback.py:136 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-03-07 22:10:57</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:31.50        </td></tr>\n",
       "<tr><td>Memory:      </td><td>10.3/16.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  combined_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>lambda_eae00_00000</td><td>TERMINATED</td><td>127.0.0.1:28531</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.00385852 </td><td style=\"text-align: right;\">          40</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        10.1791 </td><td style=\"text-align: right;\">     0.969099</td></tr>\n",
       "<tr><td>lambda_eae00_00001</td><td>TERMINATED</td><td>127.0.0.1:28534</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">0.000198153</td><td style=\"text-align: right;\">          70</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">        10.4035 </td><td style=\"text-align: right;\">     0.945395</td></tr>\n",
       "<tr><td>lambda_eae00_00002</td><td>TERMINATED</td><td>127.0.0.1:28533</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.000318458</td><td style=\"text-align: right;\">          80</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        22.2735 </td><td style=\"text-align: right;\">     0.963178</td></tr>\n",
       "<tr><td>lambda_eae00_00003</td><td>TERMINATED</td><td>127.0.0.1:28532</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.00105317 </td><td style=\"text-align: right;\">          40</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        10.1131 </td><td style=\"text-align: right;\">     0.96835 </td></tr>\n",
       "<tr><td>lambda_eae00_00004</td><td>TERMINATED</td><td>127.0.0.1:28535</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.00119516 </td><td style=\"text-align: right;\">          20</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.14591</td><td style=\"text-align: right;\">     0.958058</td></tr>\n",
       "<tr><td>lambda_eae00_00005</td><td>TERMINATED</td><td>127.0.0.1:28536</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">2.34847e-05</td><td style=\"text-align: right;\">          90</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">        11.6371 </td><td style=\"text-align: right;\">     0.740575</td></tr>\n",
       "<tr><td>lambda_eae00_00006</td><td>TERMINATED</td><td>127.0.0.1:28537</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">0.00250959 </td><td style=\"text-align: right;\">          60</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         9.03338</td><td style=\"text-align: right;\">     0.96975 </td></tr>\n",
       "<tr><td>lambda_eae00_00007</td><td>TERMINATED</td><td>127.0.0.1:28538</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">1.33953e-05</td><td style=\"text-align: right;\">          60</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         8.89883</td><td style=\"text-align: right;\">     0.617266</td></tr>\n",
       "<tr><td>lambda_eae00_00008</td><td>TERMINATED</td><td>127.0.0.1:28579</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">0.000335611</td><td style=\"text-align: right;\">          60</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         4.69758</td><td style=\"text-align: right;\">     0.95468 </td></tr>\n",
       "<tr><td>lambda_eae00_00009</td><td>TERMINATED</td><td>127.0.0.1:28591</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.000254012</td><td style=\"text-align: right;\">          70</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">        14.2119 </td><td style=\"text-align: right;\">     0.966424</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th style=\"text-align: right;\">  combined_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>lambda_eae00_00000</td><td style=\"text-align: right;\">     0.969099</td></tr>\n",
       "<tr><td>lambda_eae00_00001</td><td style=\"text-align: right;\">     0.945395</td></tr>\n",
       "<tr><td>lambda_eae00_00002</td><td style=\"text-align: right;\">     0.963178</td></tr>\n",
       "<tr><td>lambda_eae00_00003</td><td style=\"text-align: right;\">     0.96835 </td></tr>\n",
       "<tr><td>lambda_eae00_00004</td><td style=\"text-align: right;\">     0.958058</td></tr>\n",
       "<tr><td>lambda_eae00_00005</td><td style=\"text-align: right;\">     0.740575</td></tr>\n",
       "<tr><td>lambda_eae00_00006</td><td style=\"text-align: right;\">     0.96975 </td></tr>\n",
       "<tr><td>lambda_eae00_00007</td><td style=\"text-align: right;\">     0.617266</td></tr>\n",
       "<tr><td>lambda_eae00_00008</td><td style=\"text-align: right;\">     0.95468 </td></tr>\n",
       "<tr><td>lambda_eae00_00009</td><td style=\"text-align: right;\">     0.966424</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 22:10:57,793\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/choonkeatling/ray_results/lambda_2025-03-07_22-10-26' in 0.0038s.\n",
      "2025-03-07 22:10:57,796\tINFO tune.py:1041 -- Total run time: 31.54 seconds (31.50 seconds for the tuning loop).\n",
      "2025-03-07 22:10:57,812\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2025-03-07 22:10:57,841\tWARNING callback.py:136 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best hyperparameters for DistilBERT:\n",
      "{'lr': 0.0025095931971985054, 'batch_size': 64, 'num_epochs': 60}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-03-07 22:11:32</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:34.97        </td></tr>\n",
       "<tr><td>Memory:      </td><td>10.1/16.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  combined_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>lambda_fdaf1_00000</td><td>TERMINATED</td><td>127.0.0.1:28630</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">3.49922e-05</td><td style=\"text-align: right;\">          80</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        23.5591 </td><td style=\"text-align: right;\">     0.881303</td></tr>\n",
       "<tr><td>lambda_fdaf1_00001</td><td>TERMINATED</td><td>127.0.0.1:28627</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.000767218</td><td style=\"text-align: right;\">          50</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        11.3269 </td><td style=\"text-align: right;\">     0.966105</td></tr>\n",
       "<tr><td>lambda_fdaf1_00002</td><td>TERMINATED</td><td>127.0.0.1:28628</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">8.00534e-05</td><td style=\"text-align: right;\">          50</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         7.10104</td><td style=\"text-align: right;\">     0.789879</td></tr>\n",
       "<tr><td>lambda_fdaf1_00003</td><td>TERMINATED</td><td>127.0.0.1:28629</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.00615873 </td><td style=\"text-align: right;\">          10</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         5.33175</td><td style=\"text-align: right;\">     0.973078</td></tr>\n",
       "<tr><td>lambda_fdaf1_00004</td><td>TERMINATED</td><td>127.0.0.1:28634</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.00225477 </td><td style=\"text-align: right;\">          70</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">        14.3072 </td><td style=\"text-align: right;\">     0.97297 </td></tr>\n",
       "<tr><td>lambda_fdaf1_00005</td><td>TERMINATED</td><td>127.0.0.1:28633</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.000165867</td><td style=\"text-align: right;\">          30</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">        12.5082 </td><td style=\"text-align: right;\">     0.945111</td></tr>\n",
       "<tr><td>lambda_fdaf1_00006</td><td>TERMINATED</td><td>127.0.0.1:28631</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">3.66624e-05</td><td style=\"text-align: right;\">          40</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         9.01927</td><td style=\"text-align: right;\">     0.703782</td></tr>\n",
       "<tr><td>lambda_fdaf1_00007</td><td>TERMINATED</td><td>127.0.0.1:28632</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.00490586 </td><td style=\"text-align: right;\">          80</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        23.7803 </td><td style=\"text-align: right;\">     0.971829</td></tr>\n",
       "<tr><td>lambda_fdaf1_00008</td><td>TERMINATED</td><td>127.0.0.1:28659</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.000827938</td><td style=\"text-align: right;\">          70</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">        10.377  </td><td style=\"text-align: right;\">     0.970229</td></tr>\n",
       "<tr><td>lambda_fdaf1_00009</td><td>TERMINATED</td><td>127.0.0.1:28670</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">2.40694e-05</td><td style=\"text-align: right;\">          90</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">        19.8695 </td><td style=\"text-align: right;\">     0.820583</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th style=\"text-align: right;\">  combined_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>lambda_fdaf1_00000</td><td style=\"text-align: right;\">     0.881303</td></tr>\n",
       "<tr><td>lambda_fdaf1_00001</td><td style=\"text-align: right;\">     0.966105</td></tr>\n",
       "<tr><td>lambda_fdaf1_00002</td><td style=\"text-align: right;\">     0.789879</td></tr>\n",
       "<tr><td>lambda_fdaf1_00003</td><td style=\"text-align: right;\">     0.973078</td></tr>\n",
       "<tr><td>lambda_fdaf1_00004</td><td style=\"text-align: right;\">     0.97297 </td></tr>\n",
       "<tr><td>lambda_fdaf1_00005</td><td style=\"text-align: right;\">     0.945111</td></tr>\n",
       "<tr><td>lambda_fdaf1_00006</td><td style=\"text-align: right;\">     0.703782</td></tr>\n",
       "<tr><td>lambda_fdaf1_00007</td><td style=\"text-align: right;\">     0.971829</td></tr>\n",
       "<tr><td>lambda_fdaf1_00008</td><td style=\"text-align: right;\">     0.970229</td></tr>\n",
       "<tr><td>lambda_fdaf1_00009</td><td style=\"text-align: right;\">     0.820583</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 22:11:32,817\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/choonkeatling/ray_results/lambda_2025-03-07_22-10-57' in 0.0040s.\n",
      "2025-03-07 22:11:32,820\tINFO tune.py:1041 -- Total run time: 35.01 seconds (34.97 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best hyperparameters for SBERT:\n",
      "{'lr': 0.006158726672413405, 'batch_size': 16, 'num_epochs': 10}\n"
     ]
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "\n",
    "def train_model(config, embedding):\n",
    "    # Assume you're working with the DistilBERT embeddings\n",
    "    data = embedding_data[embedding]\n",
    "    # Convert to dense if needed\n",
    "    X_train = data[\"X_train\"].toarray() if scipy.sparse.issparse(data[\"X_train\"]) else data[\"X_train\"]\n",
    "    X_val = data[\"X_val\"].toarray() if scipy.sparse.issparse(data[\"X_val\"]) else data[\"X_val\"]\n",
    "\n",
    "    # Instantiate the model with hyperparameters from config.\n",
    "    model = MutltiTaskModel(\n",
    "        X_train, \n",
    "        data[\"y1_train\"], \n",
    "        data[\"y2_train\"], \n",
    "        num_types=len(le.classes_),\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        lr=config[\"lr\"]\n",
    "    )\n",
    "    \n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train for one epoch at a time\n",
    "        model.train(num_epochs=1)\n",
    "        \n",
    "        # Evaluate on validation data after each epoch\n",
    "        (_, _, _, f1_label, _), \\\n",
    "        (_, _, _, f1_type, _) \\\n",
    "        = model.evaluate(X_val, data[\"y1_val\"], data[\"y2_val\"])\n",
    "        \n",
    "        # Create a combined metric. Adjust weights if necessary.\n",
    "        combined_f1 = (f1_label + f1_type) / 2\n",
    "        \n",
    "        # Report the combined F1 score\n",
    "        tune.report({\"combined_f1\": combined_f1})\n",
    "\n",
    "\n",
    "\n",
    "# Define the hyperparameter search space.\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-2),\n",
    "    \"batch_size\": tune.choice([16, 32, 64]),\n",
    "    \"num_epochs\": tune.choice([10, 20, 30, 40, 50, 60, 70, 80, 90, 100]),\n",
    "}\n",
    "\n",
    "# Run the tuning experiment for each embedding\n",
    "\n",
    "for embedding in embedding_data.keys():\n",
    "    analysis = tune.run(\n",
    "        lambda config: train_model(config, embedding),\n",
    "        config=config,\n",
    "        metric=\"combined_f1\",\n",
    "        mode=\"max\",\n",
    "        num_samples=10,  # Increase this number for a broader search\n",
    "    )\n",
    "\n",
    "    print(f\"\\nBest hyperparameters for {embedding}:\")\n",
    "    print(analysis.get_best_config(metric=\"combined_f1\", mode=\"max\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'estimator__C': 10, 'estimator__gamma': 'scale', 'estimator__kernel': 'rbf'}\n",
      "Best F1 Score: 0.9720999880689141\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Wrap SVC in a OneVsRestClassifier for multi-class problems\n",
    "ovr_svc = OneVsRestClassifier(SVC())\n",
    "\n",
    "# Define parameter grid with prefix 'estimator__'\n",
    "param_grid = {\n",
    "    \"estimator__C\": [0.1, 1, 10],\n",
    "    \"estimator__kernel\": [\"linear\", \"rbf\"],\n",
    "    \"estimator__gamma\": [\"scale\", \"auto\"],\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with the one-vs-rest classifier\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=ovr_svc, param_grid=param_grid, cv=5, scoring=\"f1_weighted\"\n",
    ")\n",
    "\n",
    "# Fit grid search on the training data\n",
    "grid_search.fit(embedding_data[\"SBERT\"][\"X_train\"], embedding_data[\"SBERT\"][\"y2_train\"])\n",
    "\n",
    "# Get the best parameters and the corresponding accuracy\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best F1 Score: {best_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
